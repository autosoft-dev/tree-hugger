{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using tree-hugger to Enhance CodeXGLUE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot7DoEZMTdNs"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "[CodeXGLUE](https://github.com/microsoft/CodeXGLUE) has recently been released by Microsoft Research Asia, Developer Division, and Bing. In their own words CodeXGLUE is \n",
        "\n",
        "> A benchmark dataset and open challenge for code intelligence. It includes a collection of code intelligence tasks and a platform for model evaluation and comparison. CodeXGLUE stands for General Language Understanding Evaluation benchmark for CODE. It includes 14 datasets for 10 diversified code intelligence tasks\n",
        "\n",
        "You can think of it as something similar to [ImageNet](http://image-net.org/) for Computer Vision or [GLUE](https://gluebenchmark.com/) for NLP. \n",
        "\n",
        "This dataseet consists of several previously released one. Such as GitHub [CodeSearchNet](https://github.com/github/CodeSearchNet), [Py150](https://eth-sri.github.io/py150) and many more. CodeXGLUE also includes some baseline implementation and you can check them all out in their official Github repo. \n",
        "\n",
        "So far so good!\n",
        "\n",
        "**BUT**\n",
        "\n",
        "What if you want to ammase your own Data Set. Those datasets that we mentioned before were all created from Open Source repos, over the past few years. They are important and very valuable. But they do not, and will not, reflect your very own code-base and its very own, and specific twists. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjmjNVmcWso3"
      },
      "source": [
        "## Enter tree-hugger\n",
        "\n",
        "We, at CodistAI, has recently open sourced our very own code processing library [tree-hugger](https://github.com/autosoft-dev/tree-hugger) (And we recently corssed 12K installations ðŸ¥)\n",
        "\n",
        "In this small tutorial we will show you how you can use tree-hugger to create your very own dataset from your very own code files which is very similar to the Open Source Data Set supplied by CodeXGLUE \n",
        "\n",
        "One reason for doing this can be to test the base line model in your own data and see how it performs (We are about to release `docly` a small command line tool which helps you to write function documentation for your Python code and we use the same parsing technique there as well ðŸ˜€ )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wQw_15yY1Vf"
      },
      "source": [
        "### Let's install tree-hugger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCSFPT6xYtkA",
        "outputId": "1ce199ec-96e4-434d-eea7-98161d3f4663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        }
      },
      "source": [
        "!pip install -U tree-hugger PyYAML"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tree-hugger\n",
            "  Downloading https://files.pythonhosted.org/packages/6d/26/24fb7a07b986665798f00fc02a892f97264f13b16a0f08ca2460fc78ccce/tree_hugger-0.9.2-py3-none-any.whl\n",
            "Collecting PyYAML\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276kB 4.0MB/s \n",
            "\u001b[?25hCollecting pygit2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/35/48842d925612f773ba409067326f329c0650a1a05a65722e737021f758e7/pygit2-1.3.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pytest in /usr/local/lib/python3.6/dist-packages (from tree-hugger) (3.6.4)\n",
            "Collecting tree-sitter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/52/26d11536a8fafaadabe9deeb0611abdd71e11602904f60a6debdde053e6f/tree_sitter-0.2.0.tar.gz (110kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112kB 37.1MB/s \n",
            "\u001b[?25hCollecting cached-property\n",
            "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: cffi>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pygit2->tree-hugger) (1.14.3)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->tree-hugger) (20.2.0)\n",
            "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->tree-hugger) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->tree-hugger) (8.5.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->tree-hugger) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->tree-hugger) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->tree-hugger) (50.3.0)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->tree-hugger) (1.9.0)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.4.0->pygit2->tree-hugger) (2.20)\n",
            "Building wheels for collected packages: PyYAML, tree-sitter\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=210cd124adfa8c3f0b0cc9b68589f75baa32c717349725d3adae72d671fe116a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for tree-sitter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tree-sitter: filename=tree_sitter-0.2.0-cp36-cp36m-linux_x86_64.whl size=297395 sha256=5a642588b6c6967819047de6126a183ece612cea885b1436e6ac4a4d233a44cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/a6/01/2290cc8441301a07e7717e9e03c6bbc0388f71c6bf1f7f37c1\n",
            "Successfully built PyYAML tree-sitter\n",
            "Installing collected packages: cached-property, pygit2, PyYAML, tree-sitter, tree-hugger\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.3.1 cached-property-1.5.2 pygit2-1.3.0 tree-hugger-0.9.2 tree-sitter-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7I6O9LHZBtt"
      },
      "source": [
        "### And use this command to build the necessary processing libary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bgf9OYFTBgZ",
        "outputId": "5954596c-a032-42c7-cff8-6d457a95babc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!create_libs -c python"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-09 13:28:58,710 INFO:Cloneing python repo from tree-sitter collections\n",
            "2020-10-09 13:29:10,095 INFO:Creating the library my-languages.so at /content\n",
            "2020-10-09 13:29:10,978 INFO:Finished creating library!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbRAiCdjdkqE"
      },
      "source": [
        "Now that we have all the necessary set-up done, let's download some files. For the ease of the tutorial we have created a small github example repo with a collection of files. Some of it is coming from Open Source repos and some we created as example files. \n",
        "\n",
        "Let's clone that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sVdIc1idhcj",
        "outputId": "a4e65413-7782-46e8-ab64-1a42f5bd7a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!git clone https://github.com/autosoft-dev/example-files.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'example-files'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 16 (delta 2), reused 11 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHM0NAnCeLaf"
      },
      "source": [
        "We are going to declare a small function that will help us go over each files in a nested directory tree (like the one above we cloned) and get each file at a time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cWsYhNyeIb5"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def check_out_path(target_path: Path):\n",
        "    \"\"\"\"\n",
        "    This function recursively yields all contents of a pathlib.Path object\n",
        "    \"\"\"\n",
        "    yield target_path\n",
        "    for file in target_path.iterdir():\n",
        "        if file.is_dir():\n",
        "            yield from check_out_path(file)\n",
        "        else:\n",
        "            yield file.absolute()\n",
        "\n",
        "\n",
        "def is_python_file(file_path: Path):\n",
        "  \"\"\"\n",
        "  This little function will help us to filter the result and keep only the python files\n",
        "  \"\"\"\n",
        "  return file_path.is_file() and file_path.suffix == \".py\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB6OniXlenyC",
        "outputId": "a2ae5125-aa6b-47eb-f5dd-272c58af30c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "for file_path in check_out_path(Path(\"example-files\")):\n",
        "  if is_python_file(file_path):\n",
        "    print(file_path)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/example-files/flask_files/cli.py\n",
            "/content/example-files/inner_dir/_internal_utils.py\n",
            "/content/example-files/api.py\n",
            "/content/example-files/simple_funcs/simple_funcs.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU92tn86f93x"
      },
      "source": [
        "And now, we will define another small function, which, given a string which represents Python code will tokeize that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud727MLOfyUY"
      },
      "source": [
        "from tokenize import tokenize\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "def tokenize_code_string(text):\n",
        "    code_tokens = []\n",
        "    for tok in tokenize(BytesIO(text.encode('utf-8')).readline):\n",
        "        if tok.string.strip() != \"\" and tok.string.strip() != \"utf-8\":\n",
        "            code_tokens.append(tok.string.strip().lower())\n",
        "    return code_tokens"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxp1hwkWgKOQ"
      },
      "source": [
        "That is all for the pre-processing. Let's use tree-hugger's powerful API in conjunction with those functions to define a dataset from those custom files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTQTIPvFexFo"
      },
      "source": [
        "# We first create our PythonParser object\n",
        "from tree_hugger.core import PythonParser"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrlIUSCBgecv"
      },
      "source": [
        "pp = PythonParser(library_loc=\"/content/my-languages.so\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8Eyii31gjk1"
      },
      "source": [
        "# We will now define a dict and populate it with the necessary data in a for loop\n",
        "\n",
        "final_out_put_data = {}\n",
        "\n",
        "for file_path in check_out_path(Path(\"example-files\")):\n",
        "  if is_python_file(file_path):\n",
        "    final_out_put_data[file_path.stem] = None\n",
        "    # we use one line, super convinient tree-hugger API call to get the needed data\n",
        "    if pp.parse_file(str(file_path)):\n",
        "      temp_cache = []\n",
        "      # The following call returns a dict where each key is a name of a function\n",
        "      # And each value is a tuple, (function_body, function_docstring)\n",
        "      func_and_docstr = pp.get_all_function_bodies(strip_docstr=True)\n",
        "      for func_name, (body, docstr) in func_and_docstr.items():\n",
        "        code_tokens = tokenize_code_string(body)\n",
        "        # Let's strip out all the internal comments\n",
        "        final_code_tokens = [t for t in code_tokens if not t.startswith(\"#\")]\n",
        "        # Split the first line of docstring and remove all the tripple quotes and strip white spaces and make it lower\n",
        "        docstr_tokens = docstr.split(\"\\n\")[0].strip().replace('\"\"\"', '').replace(\"'''\", \"\").lower().split()\n",
        "        temp_cache.append({\"code\": final_code_tokens, \"docstr\": docstr_tokens})\n",
        "      # Let's add the result to the final output\n",
        "      final_out_put_data[file_path.stem] = temp_cache"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i9vdl2nlLQO",
        "outputId": "7c23ac2d-6816-49e9-d47d-34ba2d5e9b93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        }
      },
      "source": [
        "# And we are DONE!\n",
        "\n",
        "final_out_put_data[\"api\"][0]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'code': ['def',\n",
              "  'request',\n",
              "  '(',\n",
              "  'method',\n",
              "  ',',\n",
              "  'url',\n",
              "  ')',\n",
              "  ':',\n",
              "  'with',\n",
              "  'sessions',\n",
              "  '.',\n",
              "  'session',\n",
              "  '(',\n",
              "  ')',\n",
              "  'as',\n",
              "  'session',\n",
              "  ':',\n",
              "  'return',\n",
              "  'session',\n",
              "  '.',\n",
              "  'request',\n",
              "  '(',\n",
              "  'method',\n",
              "  '=',\n",
              "  'method',\n",
              "  ',',\n",
              "  'url',\n",
              "  '=',\n",
              "  'url',\n",
              "  ',',\n",
              "  '**',\n",
              "  'kwargs',\n",
              "  ')'],\n",
              " 'docstr': ['constructs',\n",
              "  'and',\n",
              "  'sends',\n",
              "  'a',\n",
              "  ':class:`request',\n",
              "  '<request>`.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM38KwJ5mzH0"
      },
      "source": [
        "With this code, you can very easily create a dataset out of your own code files and then test the baseline models against it. \n",
        "\n",
        "\n",
        "That was easy!"
      ]
    }
  ]
}